{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3537,
     "status": "ok",
     "timestamp": 1627650680538,
     "user": {
      "displayName": "이훈민",
      "photoUrl": "",
      "userId": "00157268181097362793"
     },
     "user_tz": 240
    },
    "id": "cg7gm5FVyA3r"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import keras\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "mod = sys.modules[__name__]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 204,
     "status": "ok",
     "timestamp": 1627650691336,
     "user": {
      "displayName": "이훈민",
      "photoUrl": "",
      "userId": "00157268181097362793"
     },
     "user_tz": 240
    },
    "id": "-gn3HoNnyGdo"
   },
   "outputs": [],
   "source": [
    "def distribution_check(dataset):\n",
    "        #분포가 다름;\n",
    "    one,two,three,four,five,six,seven,eight,nine,ten=0,0,0,0,0,0,0,0,0,0\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        if(dataset[i]==1):    one+=1\n",
    "        elif(dataset[i]==2):  two+=1\n",
    "        elif(dataset[i]==3):  three+=1\n",
    "        elif(dataset[i]==4):  four+=1\n",
    "        elif(dataset[i]==5):  five+=1\n",
    "        elif(dataset[i]==6):  six+=1\n",
    "        elif(dataset[i]==7):  seven+=1\n",
    "        elif(dataset[i]==8):  eight+=1\n",
    "        elif(dataset[i]==9):  nine+=1\n",
    "        elif(dataset[i]==0):  ten+=1\n",
    "\n",
    "    #print(one,two,three,four,five,six,seven,eight,nine,ten)\n",
    "    #print(\"Sum : \", one+two+three+four+five+six+seven+eight+nine+ten)\n",
    "    sums = one+two+three+four+five+six+seven+eight+nine+ten\n",
    "    for_graph=[one,two,three,four,five,six,seven,eight,nine,ten]\n",
    "    \n",
    "    return for_graph, sums\n",
    "\n",
    "def local_model_generate(model): #초기 모델 생성\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def step_function(data):\n",
    "    if data > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def sigmoid(data):\n",
    "    return 1/(1+np.exp(-data))\n",
    "\n",
    "def relu(data):\n",
    "    return np.maximum(0,data)\n",
    "\n",
    "def random_check(local, train_data, test_data):\n",
    "    check_distribution_train_data, check_distribution_test_data, y_train_tmp, y_test_tmp = [], [], [], []\n",
    "    sum1, sum2, data_index = 0,0, np.arange(0,10)\n",
    "\n",
    "    for j in range(len(train_data)):\n",
    "        y_train_tmp.append(np.argmax(train_data[j])) #원핫인코딩에서 다시 0~9 레이블로 변환\n",
    "\n",
    "    for j in range(len(test_data)):\n",
    "        y_test_tmp.append(np.argmax(test_data[j]))\n",
    "\n",
    "    check_distribution_train_data, sum_train = distribution_check(y_train_tmp) #한번 밖에 실행 못함\n",
    "    check_distribution_test_data, sum_test = distribution_check(y_test_tmp)\n",
    "    sum1, sum2 = (sum1 + sum_train), (sum2 + sum_test)\n",
    "    \n",
    "    return check_distribution_train_data, check_distribution_test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1033,
     "status": "ok",
     "timestamp": 1627650692487,
     "user": {
      "displayName": "이훈민",
      "photoUrl": "",
      "userId": "00157268181097362793"
     },
     "user_tz": 240
    },
    "id": "CSRQl-noyHx9",
    "outputId": "a617876a-a77b-4ed9-d221-bab881835ad7"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print(x_train.shape, x_test.shape)\n",
    "num_train, num_test = len(x_train),  len(x_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1627650692488,
     "user": {
      "displayName": "이훈민",
      "photoUrl": "",
      "userId": "00157268181097362793"
     },
     "user_tz": 240
    },
    "id": "YvgMHy1ByI7c"
   },
   "outputs": [],
   "source": [
    "local = 3 # 10개 : 1~10 => 11은 포함 안됨\n",
    "batch_size, epochs = 32, 3\n",
    "global_epoch = 0\n",
    "num_layers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 116,
     "status": "ok",
     "timestamp": 1627650692602,
     "user": {
      "displayName": "이훈민",
      "photoUrl": "",
      "userId": "00157268181097362793"
     },
     "user_tz": 240
    },
    "id": "NtlWlG2EyKwJ",
    "outputId": "09f4914d-6e8c-4c05-a149-93a272cd997d"
   },
   "outputs": [],
   "source": [
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "print(\"Step 1 x_train shape:\", x_train.shape)\n",
    "\n",
    "#x_train = x_train.reshape((num_train, 32, 32, 3))\n",
    "#x_test = x_test.reshape((num_test, 32, 32, 3))\n",
    "\n",
    "print(\"Step 2 x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "# convert class vectors to binary class matrices => one hot encoding 지금은 [3,6,2,5,4,8..] 섞여있음 \n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1627650695591,
     "user": {
      "displayName": "이훈민",
      "photoUrl": "",
      "userId": "00157268181097362793"
     },
     "user_tz": 240
    },
    "id": "I2uyTUGiyaSd",
    "outputId": "771ec14e-3e68-401c-b085-0e0f38c6561f"
   },
   "outputs": [],
   "source": [
    "#랜덤 비복원추출, 동등하게 10% 씩 =  로컬 데이터 추출 ------------------------------------------------------------------------------\n",
    "\n",
    "global_epoch = 0\n",
    "\n",
    "local = 3\n",
    "\n",
    "print(global_epoch, \" 번째 global_epoch 데이터 랜덤하게 준비!!\")\n",
    "\n",
    "for i in range(1,local): #데이터 변수 선언, 빈 리스트로 초기화\n",
    "    globals()['L{}_x_train'.format(i)], globals()['L{}_x_test'.format(i)] = [], []  \n",
    "    globals()['L{}_y_train'.format(i)], globals()['L{}_y_test'.format(i)] = [], []  \n",
    "\n",
    "x_train_range = list(np.arange(0,len(x_train)))  #0~59,999\n",
    "x_test_range = list(np.arange(0,len(x_test)))  #0~9,999\n",
    "\n",
    "for n in range(1,local): #Data_x_train_range, 잘 작동함 => 확인 완료\n",
    "    tmp, tmp2 = [], []\n",
    "    tmp = random.sample(list(x_train_range), int(num_train/(local-1)))  #1/10개 만큼 인덱스 랜덤 비복원 추출 => 랜덤으로 해야함\n",
    "    tmp2 = random.sample(list(x_test_range), int(num_test/(local-1)))\n",
    "\n",
    "    for i in range(len(tmp)):\n",
    "        globals()['L{}_x_train'.format(n)].append(x_train[tmp[i]])  #n번째 Cluster에 분할한 실제 x_train 데이터 저장\n",
    "        globals()['L{}_y_train'.format(n)].append(y_train[tmp[i]]) \n",
    "        #x_train_range.remove(tmp[i])  #랜덤 하게 뽑힌 원소 6000개 추출했으니 안에서 삭제 => 비복원 추출\n",
    "\n",
    "    for j in range(len(tmp2)):\n",
    "        globals()['L{}_x_test'.format(n)].append(x_test[tmp2[j]])\n",
    "        globals()['L{}_y_test'.format(n)].append(y_test[tmp2[j]])\n",
    "        #x_test_range.remove(tmp2[j])  #랜덤 하게 뽑힌 원소 6000개 추출했으니 안에서 삭제 => 비복원 추출\n",
    "\n",
    "print(\"check=>: should be 0 == \", len(x_train_range))  #원소 하나도 없어야 함. 확인.\n",
    "\n",
    "print(global_epoch, \" 번째 global_epoch 데이터 랜덤 준비완료!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33221,
     "status": "ok",
     "timestamp": 1627650734548,
     "user": {
      "displayName": "이훈민",
      "photoUrl": "",
      "userId": "00157268181097362793"
     },
     "user_tz": 240
    },
    "id": "LztCjgrY1OxS",
    "outputId": "2c591e5f-b79f-4181-c657-5ceeee3b0839"
   },
   "outputs": [],
   "source": [
    "#준비된 데이터 (IID / NON-IID) np.array로\n",
    "\n",
    "\n",
    "global_epoch = 0\n",
    "FedAvg_ACC, FedAvg_LOSS, fedavg_acc, fedavg_loss, acc, loss = [], [], [], [], 0, 0\n",
    "num_layers_list = [0,2,4,6,7]\n",
    "label_list = []\n",
    "\n",
    "VAR_final_list_w, VAR_final_list_b = [], []\n",
    "\n",
    "\n",
    "for i in range(1,local):\n",
    "    globals()['L{}_x_train'.format(i)] = np.array(globals()['L{}_x_train'.format(i)])\n",
    "    globals()['L{}_x_test'.format(i)] = np.array(globals()['L{}_x_test'.format(i)])\n",
    "    globals()['L{}_y_train'.format(i)] = np.array(globals()['L{}_y_train'.format(i)])\n",
    "    globals()['L{}_y_test'.format(i)] = np.array(globals()['L{}_y_test'.format(i)])\n",
    "\n",
    "\n",
    "print(\"\\n\\n\",global_epoch, \"번째 global epoch에서 로컬 클라이언트 학습시작!\\n\\n학습 중 ...\\n\")\n",
    "now = time.time()\n",
    "\n",
    "for i in range(1,local):\n",
    "    globals()['FedAvg_L{}_iter{}_model'.format(i, global_epoch)] = models.Sequential()  ##!!!!!!! 이게 글로벌 epoch에서는 2번째 부터 들어가면 안됨\n",
    "    globals()['FedAvg_L{}_iter{}_model'.format(i, global_epoch)] = local_model_generate(globals()['FedAvg_L{}_iter{}_model'.format(i, global_epoch)])      # initialize 필요 => 모델 프레임 구축\n",
    "\n",
    "    globals()['FedAvg_L{}_iter{}_model'.format(i, global_epoch)].compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    globals()['FedAvg_L{}_iter{}_model'.format(i, global_epoch)].fit(globals()['L{}_x_train'.format(i)], globals()['L{}_y_train'.format(i)], batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=2)\n",
    "\n",
    "    loss, acc = globals()['FedAvg_L{}_iter{}_model'.format(i, global_epoch)].evaluate(globals()['L{}_x_test'.format(i,global_epoch)], globals()['L{}_y_test'.format(i)], verbose=2)\n",
    "    \n",
    "    fedavg_acc.append(acc)  #acc 넣기\n",
    "    fedavg_loss.append(loss)  #loss 넣기\n",
    "\n",
    "\n",
    "print(global_epoch, \"번째 global epoch 로컬 클라이언트 학습완료!,  Total Training time : \", time.time()-now,\"\\n\\n\")\n",
    "\n",
    "# 로컬 모델들 학습 완료하였고 로컬모델에서 weight, bias 추출 -------------------------------------------------------------------------------------\n",
    "\n",
    "for i in range(1, local):\n",
    "    for layer_index in num_layers_list:\n",
    "        w = globals()['FedAvg_L{}_iter{}_model'.format(i, global_epoch)].layers[layer_index].get_weights() #get_weights = w[0],b[1]로 구성\n",
    "\n",
    "        globals()['L{}_layer{}_w_tmp'.format(i,layer_index)] = w      # 클러스터링 용도\n",
    "        globals()['L{}_layer{}_w'.format(i,layer_index)] = w          # weight aggregation 용도\n",
    "            \n",
    "# 클러스터에 넣을 변수 선언\n",
    "\n",
    "for layer_index in num_layers_list:\n",
    "\n",
    "    globals()['layer{}_W_tmp'.format(layer_index)] = []\n",
    "    globals()['G{}_w_layer{}'.format(global_epoch, layer_index)] = 0  #FedAvg 용도\n",
    "    \n",
    "    globals()['var_list_layer{}_w'.format(layer_index)] = []\n",
    "    globals()['var_list_layer{}_b'.format(layer_index)] = []\n",
    "  \n",
    "    w_scaler = 0\n",
    "    #------------------------------------------------FedAvg--------------------------------------------------------\n",
    "    for i in range(1, local):\n",
    "        globals()['var_list_layer{}_w'.format(layer_index)].append(np.var(globals()['L{}_layer{}_w'.format(i,layer_index)][0]))\n",
    "        globals()['var_list_layer{}_b'.format(layer_index)].append(np.var(globals()['L{}_layer{}_w'.format(i,layer_index)][1]))\n",
    "      \n",
    "        w_scaler = len(globals()['L{}_x_train'.format(i)]) / 50000\n",
    "        globals()['G{}_w_layer{}'.format(global_epoch, layer_index)] = globals()['G{}_w_layer{}'.format(global_epoch, layer_index)] + (np.array(globals()['L{}_layer{}_w'.format(i,layer_index)]) * w_scaler)\n",
    "    \n",
    "    globals()['G{}_w_layer{}'.format(global_epoch, layer_index)] = np.array(globals()['G{}_w_layer{}'.format(global_epoch, layer_index)]) / (local-1)   #np.array로 변함\n",
    "    \n",
    "    VAR_final_list_w.append(globals()['var_list_layer{}_w'.format(layer_index)])\n",
    "    VAR_final_list_b.append(globals()['var_list_layer{}_b'.format(layer_index)])\n",
    "    \n",
    "#---------------------------------------------------------메모리 삭제-------------------------------------------------------------------\n",
    "\n",
    "FedAvg_ACC.append(fedavg_acc)\n",
    "FedAvg_LOSS.append(fedavg_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6GDN301TefI"
   },
   "outputs": [],
   "source": [
    "G_epoch = 3\n",
    "\n",
    "for global_epoch in range(1, G_epoch):\n",
    "\n",
    "    for layer_index in num_layers_list:\n",
    "        for i in range(1, local):  #FedAvg\n",
    "            globals()['FedAvg_L{}_iter{}_model'.format(i,global_epoch)] = globals()['FedAvg_L{}_iter{}_model'.format(i,global_epoch-1)]   #구조만 필요\n",
    "\n",
    "            globals()['FedAvg_L{}_iter{}_model'.format(i,global_epoch)].layers[layer_index].set_weights(globals()['G{}_w_layer{}'.format(global_epoch-1, layer_index)])  #set_weights\n",
    "        \n",
    "\n",
    "    #----------------------------------------------------------------데이터 새로 준비----------------------------------------------------------------------------------\n",
    "    print(global_epoch, \" 번째 global_epoch 데이터 랜덤하게 준비!!\")\n",
    "\n",
    "\n",
    "    for i in range(1,local):\n",
    "        globals()['L{}_x_train'.format(i)], globals()['L{}_x_test'.format(i)] = [], []\n",
    "        globals()['L{}_y_train'.format(i)], globals()['L{}_y_test'.format(i)] = [], []  \n",
    "        \n",
    "    x_train_range = list(np.arange(0,len(x_train)))  #0~59,999\n",
    "    x_test_range = list(np.arange(0,len(x_test)))  #0~9,999\n",
    "\n",
    "    for n in range(1,local): #Data_x_train_range, 잘 작동함 => 확인 완료\n",
    "        tmp, tmp2 = [], []\n",
    "        tmp = random.sample(list(x_train_range), int(num_train/(local-1)))  #1/10개 만큼 인덱스 랜덤 비복원 추출 => 랜덤으로 해야함\n",
    "        tmp2 = random.sample(list(x_test_range), int(num_test/(local-1)))\n",
    "\n",
    "        for i in range(len(tmp)):\n",
    "            globals()['L{}_x_train'.format(n)].append(x_train[tmp[i]])  #n번째 Cluster에 분할한 실제 x_train 데이터 저장\n",
    "            globals()['L{}_y_train'.format(n)].append(y_train[tmp[i]]) \n",
    "            #x_train_range.remove(tmp[i])  #랜덤 하게 뽑힌 원소 6000개 추출했으니 안에서 삭제 = 복원추출\n",
    "\n",
    "        for j in range(len(tmp2)):\n",
    "            globals()['L{}_x_test'.format(n)].append(x_test[tmp2[j]])\n",
    "            globals()['L{}_y_test'.format(n)].append(y_test[tmp2[j]])\n",
    "            #x_test_range.remove(tmp2[j])  #랜덤 하게 뽑힌 원소 6000개 추출했으니 안에서 삭제\n",
    "\n",
    "    for i in range(1,local):\n",
    "        globals()['L{}_x_train'.format(i)] = np.array(globals()['L{}_x_train'.format(i)])\n",
    "        globals()['L{}_x_test'.format(i)] = np.array(globals()['L{}_x_test'.format(i)])\n",
    "        globals()['L{}_y_train'.format(i)] = np.array(globals()['L{}_y_train'.format(i)])\n",
    "        globals()['L{}_y_test'.format(i)] = np.array(globals()['L{}_y_test'.format(i)])\n",
    "\n",
    "\n",
    "    print(global_epoch, \" 번째 global_epoch 데이터 랜덤 준비완료!!\\n\\n\")\n",
    "\n",
    "    print(global_epoch, \" 번째 global epoch에서 로컬 클라이언트 들 학습시작!\\n\\n학습 중 ...\\n\")\n",
    "    \n",
    "\n",
    "    #-------------------------------------------------------------------여기가 핵심, 알고리즘 3개 따로 학습시켜야 함--------------------------------------------------------------------\n",
    "\n",
    "    #======================================================================== FedAvg ========================================================================\n",
    "\n",
    "    fedavg_acc, fedavg_loss, now = [],[], time.time()\n",
    "\n",
    "    for i in range(1,local):\n",
    "        # 아까 옮겨줘서 모델 그대로 compile, train 시키면 됨      \n",
    "        globals()['FedAvg_L{}_iter{}_model'.format(i, global_epoch)].compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "        globals()['FedAvg_L{}_iter{}_model'.format(i, global_epoch)].fit(globals()['L{}_x_train'.format(i)], globals()['L{}_y_train'.format(i)], batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=0)\n",
    "\n",
    "        loss, acc = globals()['FedAvg_L{}_iter{}_model'.format(i, global_epoch)].evaluate(globals()['L{}_x_test'.format(i)], globals()['L{}_y_test'.format(i)], verbose=2)\n",
    "        \n",
    "        fedavg_acc.append(acc)  #acc 넣기\n",
    "        fedavg_loss.append(loss)  #loss 넣기\n",
    "\n",
    "    print(\"------------   \", global_epoch, \" 번째 global epoch < FedAVG > 로컬 클라이언트 학습 완료!,  Total Training time : \", time.time()-now,\"---------------------------\\n\\n\")\n",
    "\n",
    "\n",
    "# 로컬 모델들 학습 완료, 로컬모델에서 weight, bias 추출 -------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    for i in range(1, local):        \n",
    "        for layer_index in num_layers_list:\n",
    "            globals()['L{}_layer{}_w_tmp'.format(i,layer_index)] = globals()['FedAvg_L{}_iter{}_model'.format(i, global_epoch)].layers[layer_index].get_weights() #get_weights = w[0],b[1]로 구성, FedAvg\n",
    "            \n",
    "    #------------------------------------------------FedAvg--------------------------------------------------------바로 Fedavg 시키기\n",
    "    \n",
    "    for layer_index in num_layers_list:\n",
    "        globals()['G{}_w_layer{}'.format(global_epoch, layer_index)] = 0\n",
    "\n",
    "        globals()['var_list_layer{}_w'.format(layer_index)] = []\n",
    "        globals()['var_list_layer{}_b'.format(layer_index)] = []\n",
    "        w_scaler = len(globals()['L{}_x_train'.format(i)]) / 50000\n",
    "        \n",
    "        for i in range(1, local):\n",
    "            globals()['G{}_w_layer{}'.format(global_epoch, layer_index)] = globals()['G{}_w_layer{}'.format(global_epoch, layer_index)] + (np.array(globals()['L{}_layer{}_w_tmp'.format(i,layer_index)])*w_scaler)\n",
    "\n",
    "        for i in range(1, local):\n",
    "            globals()['var_list_layer{}_w'.format(layer_index)].append(np.var(globals()['G{}_w_layer{}'.format(global_epoch, layer_index)][0]))            \n",
    "            globals()['var_list_layer{}_b'.format(layer_index)].append(np.var(globals()['G{}_w_layer{}'.format(global_epoch, layer_index)][1]))\n",
    "            \n",
    "          \n",
    "        VAR_final_list_w.append(globals()['var_list_layer{}_w'.format(layer_index)])\n",
    "        VAR_final_list_b.append(globals()['var_list_layer{}_b'.format(layer_index)])\n",
    "    \n",
    "\n",
    "    FedAvg_ACC.append(fedavg_acc)\n",
    "    FedAvg_LOSS.append(fedavg_loss)\n",
    "\n",
    "    print(\"\\n\\n====================================================== One Global Epoch =====================================================================\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 96,
     "status": "ok",
     "timestamp": 1627653247965,
     "user": {
      "displayName": "이훈민",
      "photoUrl": "",
      "userId": "00157268181097362793"
     },
     "user_tz": 240
    },
    "id": "UmEVOVgF9ExJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2XKazZ-4dgs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxtWKBvP9u38"
   },
   "source": [
    "# Var 그래프 : 누적량이랑 변화량"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 916,
     "status": "ok",
     "timestamp": 1627658316580,
     "user": {
      "displayName": "이훈민",
      "photoUrl": "",
      "userId": "00157268181097362793"
     },
     "user_tz": 240
    },
    "id": "ZubrDfX64yoX",
    "outputId": "c25360c0-8167-4d97-db29-af3f82a686ac"
   },
   "outputs": [],
   "source": [
    "var_changes_L0_w, var_changes_L2_w, var_changes_L4_w, var_changes_L6_w, var_changes_L7_w = [], [], [], [], [] \n",
    "var_accumulate_L0_w, var_accumulate_L2_w, var_accumulate_L4_w, var_accumulate_L6_w, var_accumulate_L7_w = [], [], [], [], [] \n",
    "\n",
    "var_changes_L0_b, var_changes_L2_b, var_changes_L4_b, var_changes_L6_b, var_changes_L7_b = [], [], [], [], [] \n",
    "var_accumulate_L0_b, var_accumulate_L2_b, var_accumulate_L4_b, var_accumulate_L6_b, var_accumulate_L7_b = [], [], [], [], [] \n",
    "\n",
    "\n",
    "for global_epoch in range(G_epoch-1):\n",
    "    for layer_index in num_layers_list:\n",
    "        globals()['var_accumulate_L{}_w'.format(layer_index)].append(np.var(globals()['G{}_w_layer{}'.format(global_epoch, layer_index)][0]))\n",
    "        globals()['var_changes_L{}_w'.format(layer_index)].append(np.var(globals()['G{}_w_layer{}'.format(global_epoch+1, layer_index)][0]) - np.var(globals()['G{}_w_layer{}'.format(global_epoch, layer_index)][0]))\n",
    "        \n",
    "        globals()['var_accumulate_L{}_b'.format(layer_index)].append(np.var(globals()['G{}_w_layer{}'.format(global_epoch, layer_index)][1]))\n",
    "        globals()['var_changes_L{}_b'.format(layer_index)].append(np.var(globals()['G{}_w_layer{}'.format(global_epoch+1, layer_index)][1]) - np.var(globals()['G{}_w_layer{}'.format(global_epoch, layer_index)][1]))\n",
    "        \n",
    "\n",
    "print(\"Variance Accumulate Layer 0,2,4,6,7\\n--------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "print(\"\\nVar Accumulate Weight\\n--------------------------------------\")\n",
    "for j in num_layers_list:\n",
    "    print(globals()['var_accumulate_L{}_w'.format(j)])\n",
    "\n",
    "print(\"\\nVar Accumulate Bias\\n--------------------------------------\")\n",
    "for j in num_layers_list:\n",
    "    print(globals()['var_accumulate_L{}_b'.format(j)])\n",
    "\n",
    "print(\"\\n\\nVariance Change Layer 0,2,4,6,7\\n--------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "print(\"\\nVar Changes Weight\\n--------------------------------------\")\n",
    "for j in num_layers_list:\n",
    "    print(globals()['var_accumulate_L{}_w'.format(j)])\n",
    "\n",
    "print(\"\\nVar Changes Bias\\n--------------------------------------\")\n",
    "for j in num_layers_list:\n",
    "    print(globals()['var_changes_L{}_b'.format(j)])\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nNow Check the graph\\n\")\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "for j in num_layers_list:\n",
    "    plt.plot(globals()['var_accumulate_L{}_w'.format(j)])\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "for j in num_layers_list:\n",
    "    plt.plot(globals()['var_accumulate_L{}_b'.format(j)])\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "for j in num_layers_list:\n",
    "    plt.plot(globals()['var_changes_L{}_b'.format(j)])\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "for j in num_layers_list:\n",
    "    plt.plot(globals()['var_changes_L{}_b'.format(j)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "malirJA890bg"
   },
   "source": [
    "# 글로벌 에폭 변하는 동안 레이어들간에 거리 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1627656424321,
     "user": {
      "displayName": "이훈민",
      "photoUrl": "",
      "userId": "00157268181097362793"
     },
     "user_tz": 240
    },
    "id": "NuPi97cI2DvA",
    "outputId": "d9a962bb-1b46-44a4-b1bb-ced99230dff8"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "dist_0, dist_2, dist_4, dist_6, dist_7  = [], [], [], [], []\n",
    "\n",
    "for global_epoch in range(G_epoch-1):\n",
    "    for layer_index in num_layers_list:\n",
    "        globals()['dist_{}'.format(layer_index)].append(np.mean(distance.euclidean(globals()['G{}_w_layer{}'.format(global_epoch, layer_index)], globals()['G{}_w_layer{}'.format(global_epoch+1, layer_index)])))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Distance Layer 0,2,4,6,7\\n--------------------------------------------------------------------------------------------------\")\n",
    "print(dist_0)\n",
    "print(dist_2)\n",
    "print(dist_4)\n",
    "print(dist_6)\n",
    "print(dist_7, \"\\n\\nCheck the graph\\n\")\n",
    "\n",
    "plt.plot(dist_0)\n",
    "plt.plot(dist_2)\n",
    "plt.plot(dist_4)\n",
    "plt.plot(dist_6)\n",
    "plt.plot(dist_7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DizEFVTAACiJ"
   },
   "outputs": [],
   "source": [
    "#논문에 분산 얘기 할때 조심해야 할듯. 범위를 고려한다는 표현을 붙이던지 아님 거리의 평균 변화량이라고 표현하는 게 나을 듯\n",
    "\n",
    "#중심점으로 수렴하는 것 = 거리 변화량 낮아짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "executionInfo": {
     "elapsed": 457,
     "status": "ok",
     "timestamp": 1627658437641,
     "user": {
      "displayName": "이훈민",
      "photoUrl": "",
      "userId": "00157268181097362793"
     },
     "user_tz": 240
    },
    "id": "4vSuPXEKhOVJ",
    "outputId": "a8d45582-eb41-497b-b3db-31eb45bb1597"
   },
   "outputs": [],
   "source": [
    "average_acc_list, average_loss_list = [], []\n",
    "\n",
    "\n",
    "for i in range(len(FedAvg_ACC)):\n",
    "    average_acc_list.append(np.mean(FedAvg_ACC[i]))\n",
    "    average_loss_list.append(np.mean(FedAvg_LOSS[i]))\n",
    "    \n",
    "\n",
    "print(\"Average Acc and Loss Layer 0,2,4,6,7\\n--------------------------------------------------------------------------------------------------\")\n",
    "print(FedAvg_ACC)\n",
    "print(FedAvg_LOSS)\n",
    "print(\"\\n\\n Now Check the graph\\n\")\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(average_acc_list, label='FedAvg Acc')\n",
    "\n",
    "plt.plot(average_loss_list, label='Fedavg Loss')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygRcwFcWH5Ik"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN8BwhBgOI63xua8QZ61//X",
   "collapsed_sections": [],
   "name": "Test1_paper._Fedavg.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
