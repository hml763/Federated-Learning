{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3308,
     "status": "ok",
     "timestamp": 1627827127594,
     "user": {
      "displayName": "이훈민",
      "photoUrl": "",
      "userId": "00157268181097362793"
     },
     "user_tz": 240
    },
    "id": "cg7gm5FVyA3r"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import keras\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from tensorflow.keras.datasets import cifar10 \n",
    "from keras.datasets import fashion_mnist \n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "mod = sys.modules[__name__]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1627827127595,
     "user": {
      "displayName": "이훈민",
      "photoUrl": "",
      "userId": "00157268181097362793"
     },
     "user_tz": 240
    },
    "id": "-gn3HoNnyGdo"
   },
   "outputs": [],
   "source": [
    "def distribution_check(dataset):\n",
    "        #분포가 다름;\n",
    "    one,two,three,four,five,six,seven,eight,nine,ten=0,0,0,0,0,0,0,0,0,0\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        if(dataset[i]==1):    one+=1\n",
    "        elif(dataset[i]==2):  two+=1\n",
    "        elif(dataset[i]==3):  three+=1\n",
    "        elif(dataset[i]==4):  four+=1\n",
    "        elif(dataset[i]==5):  five+=1\n",
    "        elif(dataset[i]==6):  six+=1\n",
    "        elif(dataset[i]==7):  seven+=1\n",
    "        elif(dataset[i]==8):  eight+=1\n",
    "        elif(dataset[i]==9):  nine+=1\n",
    "        elif(dataset[i]==0):  ten+=1\n",
    "\n",
    "    #print(one,two,three,four,five,six,seven,eight,nine,ten)\n",
    "    #print(\"Sum : \", one+two+three+four+five+six+seven+eight+nine+ten)\n",
    "    sums = one+two+three+four+five+six+seven+eight+nine+ten\n",
    "    for_graph=[one,two,three,four,five,six,seven,eight,nine,ten]\n",
    "    \n",
    "    return for_graph, sums\n",
    "\n",
    "def local_model_generate(model): #초기 모델 생성\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def step_function(data):\n",
    "    if data > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def sigmoid(data):\n",
    "    return 1/(1+np.exp(-data))\n",
    "\n",
    "def relu(data):\n",
    "    return np.maximum(0,data)\n",
    "\n",
    "def random_check(local, train_data, test_data):\n",
    "    check_distribution_train_data, check_distribution_test_data, y_train_tmp, y_test_tmp = [], [], [], []\n",
    "    sum1, sum2, data_index = 0,0, np.arange(0,10)\n",
    "\n",
    "    for j in range(len(train_data)):\n",
    "        y_train_tmp.append(np.argmax(train_data[j])) #원핫인코딩에서 다시 0~9 레이블로 변환\n",
    "\n",
    "    for j in range(len(test_data)):\n",
    "        y_test_tmp.append(np.argmax(test_data[j]))\n",
    "\n",
    "    check_distribution_train_data, sum_train = distribution_check(y_train_tmp) #한번 밖에 실행 못함\n",
    "    check_distribution_test_data, sum_test = distribution_check(y_test_tmp)\n",
    "    sum1, sum2 = (sum1 + sum_train), (sum2 + sum_test)\n",
    "    \n",
    "    return check_distribution_train_data, check_distribution_test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 957,
     "status": "ok",
     "timestamp": 1627827128547,
     "user": {
      "displayName": "이훈민",
      "photoUrl": "",
      "userId": "00157268181097362793"
     },
     "user_tz": 240
    },
    "id": "CSRQl-noyHx9",
    "outputId": "b2ec7a93-8bf8-45a1-b6d4-f51e17153506"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "print(x_train.shape, x_test.shape)\n",
    "num_train, num_test = len(x_train),  len(x_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1627827128547,
     "user": {
      "displayName": "이훈민",
      "photoUrl": "",
      "userId": "00157268181097362793"
     },
     "user_tz": 240
    },
    "id": "YvgMHy1ByI7c"
   },
   "outputs": [],
   "source": [
    "local = 21 # 10개 : 1~10 => 11은 포함 안됨\n",
    "batch_size, epochs = 32, 10\n",
    "global_epoch = 0\n",
    "num_layers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 202,
     "status": "ok",
     "timestamp": 1627827128746,
     "user": {
      "displayName": "이훈민",
      "photoUrl": "",
      "userId": "00157268181097362793"
     },
     "user_tz": 240
    },
    "id": "NtlWlG2EyKwJ",
    "outputId": "bd7e6c72-1283-46a1-8001-96614e342596"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 x_train shape: (60000, 28, 28)\n",
      "Step 2 x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "print(\"Step 1 x_train shape:\", x_train.shape)\n",
    "\n",
    "x_train = x_train.reshape((num_train, 28, 28, 1))\n",
    "x_test = x_test.reshape((num_test, 28, 28, 1))\n",
    "\n",
    "print(\"Step 2 x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "# convert class vectors to binary class matrices => one hot encoding 지금은 [3,6,2,5,4,8..] 섞여있음 \n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 214,
     "status": "ok",
     "timestamp": 1627827129402,
     "user": {
      "displayName": "이훈민",
      "photoUrl": "",
      "userId": "00157268181097362793"
     },
     "user_tz": 240
    },
    "id": "I2uyTUGiyaSd",
    "outputId": "d59b69f8-078b-4eca-ee46-60c09171e176"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  번째 global_epoch 데이터 랜덤하게 준비!!\n",
      "CHECK : 1  번째 로컬 클라이언트 Number of Training sets :  4985    Number of Training sets :  476\n",
      "CHECK : 2  번째 로컬 클라이언트 Number of Training sets :  2032    Number of Training sets :  476\n",
      "CHECK : 3  번째 로컬 클라이언트 Number of Training sets :  1676    Number of Training sets :  476\n",
      "CHECK : 4  번째 로컬 클라이언트 Number of Training sets :  2057    Number of Training sets :  476\n",
      "CHECK : 5  번째 로컬 클라이언트 Number of Training sets :  2225    Number of Training sets :  476\n",
      "CHECK : 6  번째 로컬 클라이언트 Number of Training sets :  3353    Number of Training sets :  476\n",
      "CHECK : 7  번째 로컬 클라이언트 Number of Training sets :  4732    Number of Training sets :  476\n",
      "CHECK : 8  번째 로컬 클라이언트 Number of Training sets :  2477    Number of Training sets :  476\n",
      "CHECK : 9  번째 로컬 클라이언트 Number of Training sets :  2966    Number of Training sets :  476\n",
      "CHECK : 10  번째 로컬 클라이언트 Number of Training sets :  3279    Number of Training sets :  476\n",
      "CHECK : 11  번째 로컬 클라이언트 Number of Training sets :  1518    Number of Training sets :  476\n",
      "CHECK : 12  번째 로컬 클라이언트 Number of Training sets :  2029    Number of Training sets :  476\n",
      "CHECK : 13  번째 로컬 클라이언트 Number of Training sets :  2013    Number of Training sets :  476\n",
      "CHECK : 14  번째 로컬 클라이언트 Number of Training sets :  1635    Number of Training sets :  476\n",
      "CHECK : 15  번째 로컬 클라이언트 Number of Training sets :  1059    Number of Training sets :  476\n",
      "CHECK : 16  번째 로컬 클라이언트 Number of Training sets :  4262    Number of Training sets :  476\n",
      "CHECK : 17  번째 로컬 클라이언트 Number of Training sets :  2911    Number of Training sets :  476\n",
      "CHECK : 18  번째 로컬 클라이언트 Number of Training sets :  2197    Number of Training sets :  476\n",
      "CHECK : 19  번째 로컬 클라이언트 Number of Training sets :  1888    Number of Training sets :  476\n",
      "CHECK : 20  번째 로컬 클라이언트 Number of Training sets :  3533    Number of Training sets :  476\n",
      "check=>: should be 0 ==  60000\n",
      "0  번째 global_epoch 데이터 랜덤 준비완료!!\n"
     ]
    }
   ],
   "source": [
    "global_epoch = 0\n",
    "\n",
    "print(global_epoch, \" 번째 global_epoch 데이터 랜덤하게 준비!!\")\n",
    "\n",
    "for i in range(1,local): #데이터 변수 선언, 빈 리스트로 초기화\n",
    "    globals()['L{}_x_train'.format(i)], globals()['L{}_x_test'.format(i)] = [],[]  \n",
    "    globals()['L{}_y_train'.format(i)], globals()['L{}_y_test'.format(i)] = [],[]  \n",
    "\n",
    "\n",
    "for n in range(1,local): #x_train_range, 잘 작동함 => 확인 완료\n",
    "\n",
    "    x_train_range = list(np.arange(0,len(x_train)))  #0~59,999\n",
    "    x_test_range = list(np.arange(0,len(x_test)))  #0~9,999\n",
    "\n",
    "    tmp, tmp2 = [], []\n",
    "\n",
    "    num_pick = random.randint(1000, 5000 ) #변동 사항 #int(len(list(x_train_range))/local)\n",
    "    num_pick2 = int(len(x_test_range)/local) #test 데이터는 원래대로 항상 일정한 비율로 주면 될듯 => 어느정도 분량 이상 있어야 함\n",
    "    \n",
    "    tmp = random.sample(list(x_train_range), num_pick)  #1/10개 만큼 인덱스 랜덤 비복원 추출 => 랜덤으로 해야함\n",
    "    tmp2 = random.sample(list(x_test_range), num_pick2)\n",
    "\n",
    "    for i in range(len(tmp)):\n",
    "        globals()['L{}_x_train'.format(n)].append(x_train[tmp[i]])  #n번째 Cluster에 분할한 실제 x_train 데이터 저장\n",
    "        globals()['L{}_y_train'.format(n)].append(y_train[tmp[i]]) \n",
    "        #x_train_range.remove(tmp[i])  #랜덤 하게 뽑힌 원소갯수들 추출했으니 안에서 삭제  => 오래걸림\n",
    "\n",
    "    for j in range(len(tmp2)):\n",
    "        globals()['L{}_x_test'.format(n)].append(x_test[tmp2[j]])\n",
    "        globals()['L{}_y_test'.format(n)].append(y_test[tmp2[j]])\n",
    "        #x_test_range.remove(tmp2[j])  #랜덤 하게 뽑힌 원소 1000개 추출했으니 안에서 삭제  => 오래걸림\n",
    "    \n",
    "    print(\"CHECK :\", n,\" 번째 로컬 클라이언트 Number of Training sets : \", len(globals()['L{}_x_train'.format(n)]), \"   Number of Training sets : \", len(globals()['L{}_x_test'.format(n)]))\n",
    "\n",
    "print(\"check=>: should be 0 == \", len(x_train_range))  #원소 하나도 없어야 함. 확인.\n",
    "print(global_epoch, \" 번째 global_epoch 데이터 랜덤 준비완료!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58841,
     "status": "ok",
     "timestamp": 1627827188938,
     "user": {
      "displayName": "이훈민",
      "photoUrl": "",
      "userId": "00157268181097362793"
     },
     "user_tz": 240
    },
    "id": "LztCjgrY1OxS",
    "outputId": "a658a428-2e0b-433b-96e5-ccef1d95750e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 0 번째 global epoch에서 로컬 클라이언트 학습시작!\n",
      "\n",
      "학습 중 ...\n",
      "\n",
      "15/15 - 0s - loss: 0.5871 - accuracy: 0.8046\n",
      "15/15 - 0s - loss: 0.5604 - accuracy: 0.8214\n",
      "15/15 - 0s - loss: 0.5547 - accuracy: 0.7899\n",
      "15/15 - 0s - loss: 0.6620 - accuracy: 0.7542\n",
      "15/15 - 0s - loss: 0.4805 - accuracy: 0.8277\n",
      "15/15 - 0s - loss: 0.4713 - accuracy: 0.8382\n",
      "15/15 - 0s - loss: 0.4254 - accuracy: 0.8340\n",
      "15/15 - 0s - loss: 0.4232 - accuracy: 0.8466\n",
      "15/15 - 0s - loss: 0.4842 - accuracy: 0.8235\n",
      "15/15 - 0s - loss: 0.5637 - accuracy: 0.8340\n",
      "15/15 - 0s - loss: 0.6497 - accuracy: 0.7773\n",
      "15/15 - 0s - loss: 0.5981 - accuracy: 0.7689\n",
      "15/15 - 0s - loss: 0.5328 - accuracy: 0.8067\n",
      "15/15 - 0s - loss: 0.5948 - accuracy: 0.8109\n",
      "15/15 - 0s - loss: 0.6878 - accuracy: 0.7521\n",
      "15/15 - 0s - loss: 0.4481 - accuracy: 0.8319\n",
      "15/15 - 0s - loss: 0.5019 - accuracy: 0.8067\n",
      "15/15 - 0s - loss: 0.4691 - accuracy: 0.8172\n",
      "15/15 - 0s - loss: 0.5395 - accuracy: 0.7815\n",
      "15/15 - 0s - loss: 0.4552 - accuracy: 0.8424\n",
      "0 번째 global epoch 로컬 클라이언트 학습완료!,  Total Training time :  215.4697141647339 \n",
      "\n",
      "\n",
      "Layer :  0   클러스터 갯수 정해짐!  갯수 :  2 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-c56707d98c89>:110: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  globals()['G{}_c{}_w_layer{}'.format(global_epoch, num_c_w, layer_index)] = globals()['G{}_c{}_w_layer{}'.format(global_epoch, num_c_w, layer_index)] + np.array(globals()['L{}_layer{}_w'.format(i,layer_index)])          # 일반 클러스터링\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer :  2   클러스터 갯수 정해짐!  갯수 :  2 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-c56707d98c89>:110: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  globals()['G{}_c{}_w_layer{}'.format(global_epoch, num_c_w, layer_index)] = globals()['G{}_c{}_w_layer{}'.format(global_epoch, num_c_w, layer_index)] + np.array(globals()['L{}_layer{}_w'.format(i,layer_index)])          # 일반 클러스터링\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer :  4   클러스터 갯수 정해짐!  갯수 :  3 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-c56707d98c89>:110: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  globals()['G{}_c{}_w_layer{}'.format(global_epoch, num_c_w, layer_index)] = globals()['G{}_c{}_w_layer{}'.format(global_epoch, num_c_w, layer_index)] + np.array(globals()['L{}_layer{}_w'.format(i,layer_index)])          # 일반 클러스터링\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer :  6   클러스터 갯수 정해짐!  갯수 :  2 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-c56707d98c89>:110: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  globals()['G{}_c{}_w_layer{}'.format(global_epoch, num_c_w, layer_index)] = globals()['G{}_c{}_w_layer{}'.format(global_epoch, num_c_w, layer_index)] + np.array(globals()['L{}_layer{}_w'.format(i,layer_index)])          # 일반 클러스터링\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer :  7   클러스터 갯수 정해짐!  갯수 :  2 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-c56707d98c89>:110: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  globals()['G{}_c{}_w_layer{}'.format(global_epoch, num_c_w, layer_index)] = globals()['G{}_c{}_w_layer{}'.format(global_epoch, num_c_w, layer_index)] + np.array(globals()['L{}_layer{}_w'.format(i,layer_index)])          # 일반 클러스터링\n"
     ]
    }
   ],
   "source": [
    "#준비된 데이터 (IID / NON-IID) np.array로\n",
    "best_Sil_score_c = []\n",
    "Silhouette_sc_c = []\n",
    "global_epoch = 0\n",
    "acc, loss = 0, 0\n",
    "cluster_acc, cluster_loss, Cluster_ACC, Cluster_LOSS = [], [], [], [] \n",
    "num_layers_list = [0,2,4,6,7]\n",
    "label_list = []\n",
    "VAR_final_list_w, VAR_final_list_b = [], []\n",
    "\n",
    "\n",
    "for i in range(1,local):\n",
    "    globals()['L{}_x_train'.format(i)] = np.array(globals()['L{}_x_train'.format(i)])\n",
    "    globals()['L{}_x_test'.format(i)] = np.array(globals()['L{}_x_test'.format(i)])\n",
    "    globals()['L{}_y_train'.format(i)] = np.array(globals()['L{}_y_train'.format(i)])\n",
    "    globals()['L{}_y_test'.format(i)] = np.array(globals()['L{}_y_test'.format(i)])\n",
    "\n",
    "\n",
    "print(\"\\n\\n\",global_epoch, \"번째 global epoch에서 로컬 클라이언트 학습시작!\\n\\n학습 중 ...\\n\")\n",
    "now = time.time()\n",
    "\n",
    "for i in range(1,local):\n",
    "    globals()['FedAvg_L{}_iter{}_model'.format(i, global_epoch)] = models.Sequential()  ##!!!!!!! 이게 글로벌 epoch에서는 2번째 부터 들어가면 안됨\n",
    "    globals()['FedAvg_L{}_iter{}_model'.format(i, global_epoch)] = local_model_generate(globals()['FedAvg_L{}_iter{}_model'.format(i, global_epoch)])      # initialize 필요 => 모델 프레임 구축\n",
    "\n",
    "    globals()['FedAvg_L{}_iter{}_model'.format(i, global_epoch)].compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    globals()['FedAvg_L{}_iter{}_model'.format(i, global_epoch)].fit(globals()['L{}_x_train'.format(i)], globals()['L{}_y_train'.format(i)], batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=0)\n",
    "\n",
    "    loss, acc = globals()['FedAvg_L{}_iter{}_model'.format(i, global_epoch)].evaluate(globals()['L{}_x_test'.format(i,global_epoch)], globals()['L{}_y_test'.format(i)], verbose=2)\n",
    "    \n",
    "    cluster_acc.append(acc)  #acc 넣기\n",
    "    cluster_loss.append(loss)  #loss 넣기\n",
    "\n",
    "\n",
    "print(global_epoch, \"번째 global epoch 로컬 클라이언트 학습완료!,  Total Training time : \", time.time()-now,\"\\n\\n\")\n",
    "\n",
    "# 로컬 모델들 학습 완료하였고 로컬모델에서 weight, bias 추출 -------------------------------------------------------------------------------------\n",
    "\n",
    "for i in range(1, local):\n",
    "    for layer_index in num_layers_list:\n",
    "        w = globals()['FedAvg_L{}_iter{}_model'.format(i, global_epoch)].layers[layer_index].get_weights() #get_weights = w[0],b[1]로 구성\n",
    "\n",
    "        globals()['L{}_layer{}_w_tmp'.format(i,layer_index)] = w      # 클러스터링 용도\n",
    "        globals()['L{}_layer{}_w'.format(i,layer_index)] = w          # weight aggregation 용도\n",
    "            \n",
    "# 클러스터에 넣을 변수 선언\n",
    "\n",
    "for layer_index in num_layers_list:\n",
    "\n",
    "    globals()['layer{}_W_tmp'.format(layer_index)] = []\n",
    "    globals()['var_list_layer{}_w'.format(layer_index)] = []\n",
    "    globals()['var_list_layer{}_b'.format(layer_index)] = []\n",
    "  \n",
    "    for i in range(1, local):\n",
    "        globals()['var_list_layer{}_w'.format(layer_index)].append(np.var(globals()['L{}_layer{}_w'.format(i,layer_index)][0]))\n",
    "        globals()['var_list_layer{}_b'.format(layer_index)].append(np.var(globals()['L{}_layer{}_w'.format(i,layer_index)][1]))\n",
    "      \n",
    "        TMP_w = list(np.array(globals()['L{}_layer{}_w_tmp'.format(i,layer_index)][0]).reshape(-1))\n",
    "        TMP_b = list(np.array(globals()['L{}_layer{}_w_tmp'.format(i,layer_index)][1]).reshape(-1))\n",
    "        \n",
    "        TMP_w.extend(TMP_b) ##append 말고 더해야함..\n",
    "\n",
    "        TMP_w = np.array(TMP_w)\n",
    "        TMP_w = list(TMP_w.reshape(-1))\n",
    "        globals()['layer{}_W_tmp'.format(layer_index)].append(TMP_w)  #클러스터링 용도\n",
    "  \n",
    "\n",
    "    #----------------------------------------------------클러스터링 실시 -------------------------------------------------------------\n",
    "    silhoutte_score_w = []\n",
    "    transformed = globals()['layer{}_W_tmp'.format(layer_index)]\n",
    "\n",
    "    for clusters in range((int(local/2)-1)):       ####=>  이 범위 : 클러스터 갯수, 밑에 보면 cluster+2 있음 = 총 10개 => (0,11) = 10                                                       \n",
    "        kmeans_model_w = KMeans(n_clusters=clusters+2)   # 0, 1은 silhouette 에 안먹힘  \n",
    "\n",
    "        kmeans_model_w.fit(transformed)       #레이블 정의 \n",
    "\n",
    "        silhoutte_score_w.append(silhouette_score(globals()['layer{}_W_tmp'.format(layer_index)], kmeans_model_w.labels_, metric='euclidean'))  \n",
    "\n",
    "    num_clusters_for_w = np.argmax(silhoutte_score_w) \n",
    "    best_Sil_score_c.append(silhoutte_score_w[num_clusters_for_w])  # 처음에는 둘 다 넣어줌\n",
    "\n",
    "    kmeans_model_w = KMeans(n_clusters=num_clusters_for_w+2)   # 0, 1은 silhouette 에 안먹힘  \n",
    "\n",
    "    globals()['w_layer{}_label'.format(layer_index)] = kmeans_model_w.fit_predict(transformed)       # 가장 적합한 클러스터로 레이어에 속한 로컬들 레이블 정의 \n",
    "\n",
    "    globals()['G{}_num_clusters_in_layer{}_w'.format(global_epoch, layer_index)] = num_clusters_for_w + 2  #나중에 몇개로 나뉘었는지 알아야 함 = num_clusters_for_w이거 그대로 하면 밑에 저게 쓸일이 많아서 global 쓰면 코딩이 너무 번잡해짐;; \n",
    "    \n",
    "    label_list.append(globals()['w_layer{}_label'.format(layer_index)])                 \n",
    "    print(\"Layer : \", layer_index, \"  클러스터 갯수 정해짐!  갯수 : \", globals()['G{}_num_clusters_in_layer{}_w'.format(global_epoch, layer_index)], num_clusters_for_w) ########################################################******************************************\n",
    "\n",
    "    # ==========> 클러스터 갯수 정해짐\n",
    "\n",
    "\n",
    "    #----------------------------------------------- 스케일링  ---------------------------------------------------------------------\n",
    "    for num_c_w in range(num_clusters_for_w + 2): #클러스터 갯수 (num_clusters_for_w) 만큼 클러스터용 = 클러스터 갯수가 3이 나왔으면 3개를 만들어야 함\n",
    "        globals()['G{}_c{}_w_layer{}'.format(global_epoch, num_c_w, layer_index)] = 0  #각각 cluster, Act+cluster 용도  \n",
    "        \n",
    "        globals()['w_scaler_c{}_sum'.format(num_c_w)], globals()['int_c{}_w'.format(num_c_w)] = 0, 0 #초기화\n",
    "    \n",
    "        for i in range(1, local):\n",
    "            if globals()['w_layer{}_label'.format(layer_index)][i-1] == num_c_w  : # 클러스터 레이블에 맞게 (0 or 1 or 2 ..) \n",
    "                globals()['w_scaler_c{}_sum'.format(num_c_w)] = globals()['w_scaler_c{}_sum'.format(num_c_w)] + len(globals()['L{}_x_train'.format(i)])\n",
    "\n",
    "\n",
    "                globals()['L{}_layer{}_w'.format(i,layer_index)][0] = np.tanh(globals()['L{}_layer{}_w'.format(i,layer_index)][0])\n",
    "                globals()['L{}_layer{}_w'.format(i,layer_index)][1] = np.tanh(globals()['L{}_layer{}_w'.format(i,layer_index)][1])\n",
    "\n",
    "\n",
    "                globals()['G{}_c{}_w_layer{}'.format(global_epoch, num_c_w, layer_index)] = globals()['G{}_c{}_w_layer{}'.format(global_epoch, num_c_w, layer_index)] + np.array(globals()['L{}_layer{}_w'.format(i,layer_index)])          # 일반 클러스터링\n",
    "                \n",
    "                globals()['int_c{}_w'.format(num_c_w)] = globals()['int_c{}_w'.format(num_c_w)] + 1\n",
    "\n",
    "        globals()['w_scaler{}'.format(num_c_w)] = len(globals()['L{}_x_train'.format(i)]) / globals()['w_scaler_c{}_sum'.format(num_c_w)]\n",
    "        globals()['G{}_c{}_w_layer{}'.format(global_epoch, num_c_w, layer_index)] = globals()['G{}_c{}_w_layer{}'.format(global_epoch, num_c_w, layer_index)] / globals()['int_c{}_w'.format(num_c_w)] #레이어 별 클러스터 자기 갯수만큼 나눠주기=평균\n",
    "\n",
    "    \n",
    "    VAR_final_list_w.append(globals()['var_list_layer{}_w'.format(layer_index)])\n",
    "    VAR_final_list_b.append(globals()['var_list_layer{}_b'.format(layer_index)])\n",
    "\n",
    "#---------------------------------------------------------메모리 삭제-------------------------------------------------------------------\n",
    "\n",
    "transformed = 0\n",
    "Silhouette_sc_c.append(best_Sil_score_c)\n",
    "Cluster_ACC.append(cluster_acc)\n",
    "Cluster_LOSS.append(cluster_loss)\n",
    "\n",
    "## G{}_num_clusters_in_layer{}_w 에 속하는 거 말고 삭제 = 메모리 낭비\n",
    "\n",
    "# Weight, bias 합친 것\n",
    " # FedAvg                   :  G{}_w_layer\n",
    " # 일반 클러스터링          :  G{}_c{}_w_layer{}               클러스터 갯수 파악 : G{}_num_clusters_in_layer{}_w\n",
    " # Actiavtion + 클러스터링  :  G{}_c{}_w_layer{}_Act           클러스터 갯수 파악 : G{}_num_clusters_in_layer{}_w_c_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T6GDN301TefI",
    "outputId": "7c41f352-57d5-4dfd-dcc2-81cf97b5ef8c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  번째 global_epoch 데이터 랜덤하게 준비!!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c8249ea55128>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mglobals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'L{}_x_train'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#n번째 Cluster에 분할한 실제 x_train 데이터 저장\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[0mglobals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'L{}_y_train'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[1;31m#x_train_range.remove(tmp[i])  #랜덤 하게 뽑힌 원소갯수들 추출했으니 안에서 삭제  => 오래걸림\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "G_epoch = 51\n",
    "\n",
    "for global_epoch in range(1, G_epoch):\n",
    "\n",
    "    for i in range(1, local):\n",
    "        if global_epoch == 1:\n",
    "            globals()['G{}_c_L{}_model'.format(global_epoch, i)] = globals()['FedAvg_L{}_iter{}_model'.format(i,global_epoch-1)]   #모델 넣어줌 (우리는 구조만 필요한 것)\n",
    "        else:\n",
    "            globals()['G{}_c_L{}_model'.format(global_epoch, i)] = globals()['G{}_c_L{}_model'.format(global_epoch-1, i)]   #모델 넣어줌 (우리는 구조만 필요한 것)\n",
    "\n",
    "        for layer_index in num_layers_list: \n",
    "            num_c = globals()['G{}_num_clusters_in_layer{}_w'.format(global_epoch-1, layer_index)]   \n",
    "    \n",
    "            for ccc in range(num_c):\n",
    "                if globals()['w_layer{}_label'.format(layer_index)][i-1] == ccc:    #현재 로컬 레이어의 군집 결과 == ccc\n",
    "                    globals()['G{}_c_L{}_model'.format(global_epoch, i)].layers[layer_index].set_weights(globals()['G{}_c{}_w_layer{}'.format(global_epoch-1, ccc, layer_index) ])  # 클러스터용 로컬 모델 선언\n",
    "                    \n",
    "                  \n",
    "        \n",
    "\n",
    "    #----------------------------------------------------------------데이터 새로 준비----------------------------------------------------------------------------------\n",
    "    print(global_epoch, \" 번째 global_epoch 데이터 랜덤하게 준비!!\")\n",
    "\n",
    "    for n in range(1,local): #x_train_range, 잘 작동함 => 확인 완료\n",
    "        x_train_range = list(np.arange(0,len(x_train)))  #0~59,999\n",
    "        x_test_range = list(np.arange(0,len(x_test)))  #0~9,999\n",
    "\n",
    "        tmp, tmp2 = [], []\n",
    "\n",
    "        num_pick = random.randint(1000, int(len(list(x_train_range))/local) ) #변동 사항\n",
    "        num_pick2 = int(len(x_test_range)/local) #test 데이터는 원래대로 항상 일정한 비율로 주면 될듯 => 어느정도 분량 이상 있어야 함\n",
    "\n",
    "        tmp = random.sample(list(x_train_range), num_pick)  #1/10개 만큼 인덱스 랜덤 비복원 추출 => 랜덤으로 해야함\n",
    "        tmp2 = random.sample(list(x_test_range), num_pick2)\n",
    "\n",
    "        for i in range(len(tmp)):\n",
    "            globals()['L{}_x_train'.format(n)].append(x_train[tmp[i]])  #n번째 Cluster에 분할한 실제 x_train 데이터 저장\n",
    "            globals()['L{}_y_train'.format(n)].append(y_train[tmp[i]]) \n",
    "            #x_train_range.remove(tmp[i])  #랜덤 하게 뽑힌 원소갯수들 추출했으니 안에서 삭제  => 오래걸림\n",
    "\n",
    "        for j in range(len(tmp2)):\n",
    "            globals()['L{}_x_test'.format(n)].append(x_test[tmp2[j]])\n",
    "            globals()['L{}_y_test'.format(n)].append(y_test[tmp2[j]])\n",
    "            #x_test_range.remove(tmp2[j])  #랜덤 하게 뽑힌 원소 1000개 추출했으니 안에서 삭제  => 오래걸림\n",
    "\n",
    "    for i in range(1,local):\n",
    "        globals()['L{}_x_train'.format(i)] = np.array(globals()['L{}_x_train'.format(i)])\n",
    "        globals()['L{}_x_test'.format(i)] = np.array(globals()['L{}_x_test'.format(i)])\n",
    "        globals()['L{}_y_train'.format(i)] = np.array(globals()['L{}_y_train'.format(i)])\n",
    "        globals()['L{}_y_test'.format(i)] = np.array(globals()['L{}_y_test'.format(i)])\n",
    "\n",
    "\n",
    "    print(global_epoch, \" 번째 global_epoch 데이터 랜덤 준비완료!!\\n\\n\")\n",
    "\n",
    "    print(global_epoch, \" 번째 global epoch에서 로컬 클라이언트 들 학습시작!\\n\\n학습 중 ...\\n\")\n",
    "    \n",
    "\n",
    "    #-------------------------------------------------------------------여기가 핵심, 알고리즘 3개 따로 학습시켜야 함--------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    #======================================================================== Cluster  ========================================================================  같이 했음. 왜냐면 cluster 하고 나서 activation 을 씌우는 거라 cluster까지는 같음\n",
    "\n",
    "    cluster_acc, cluster_loss, cluster_act_acc, cluster_act_loss, now = [], [], [], [], time.time()\n",
    "    \n",
    "    for i in range(1, local):\n",
    "        globals()['G{}_c_L{}_model'.format(global_epoch, i)].compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])  #Cluster => 알맞는 클러스터에 넣기\n",
    "        globals()['G{}_c_L{}_model'.format(global_epoch, i)].fit(globals()['L{}_x_train'.format(i)], globals()['L{}_y_train'.format(i)], batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=0)\n",
    "        loss, acc = globals()['G{}_c_L{}_model'.format(global_epoch, i)].evaluate(globals()['L{}_x_test'.format(i)], globals()['L{}_y_test'.format(i)], verbose=2)\n",
    "        \n",
    "        cluster_acc.append(acc)  #acc 넣기\n",
    "        cluster_loss.append(loss)  #loss 넣기\n",
    "\n",
    "    print(\"------------   \", global_epoch, \" 번째 global epoch < Clustering > 로컬 클라이언트 학습 완료!,  Total Training time : \", time.time()-now,\"---------------------------\\n\\n\")\n",
    "\n",
    "\n",
    "# 로컬 모델들 학습 완료, 로컬모델에서 weight, bias 추출 -------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    for i in range(1, local):        \n",
    "        for layer_index in num_layers_list:\n",
    "            \n",
    "            globals()['L{}_layer{}_w_c'.format(i,layer_index)]   = globals()['G{}_c_L{}_model'.format(global_epoch, i)].layers[layer_index].get_weights() # Clustering \n",
    "            globals()['L{}_layer{}_w_c_tmp'.format(i,layer_index)]   = globals()['G{}_c_L{}_model'.format(global_epoch, i)].layers[layer_index].get_weights() # Clustering \n",
    "\n",
    "    #------------------------------------------------ 클러스터링만 한 값들 Clustering --------------------------------------------------------\n",
    "    best_Sil_score_c = []\n",
    "    for layer_index in num_layers_list:\n",
    "        globals()['layer{}_W_c_tmp'.format(layer_index)] = []\n",
    "        \n",
    "        globals()['var_list_layer{}_w'.format(layer_index)] = []\n",
    "        globals()['var_list_layer{}_b'.format(layer_index)] = []\n",
    "            \n",
    "        for i in range(1, local):\n",
    "            TMP_w = list(np.array(globals()['L{}_layer{}_w_c_tmp'.format(i,layer_index)][0]).reshape(-1))\n",
    "            TMP_b = list(np.array(globals()['L{}_layer{}_w_c_tmp'.format(i,layer_index)][1]).reshape(-1))\n",
    "            \n",
    "            TMP_w.extend(TMP_b) ##append 말고 더해야함..\n",
    "\n",
    "            TMP_w = np.array(TMP_w)\n",
    "            TMP_w = list(TMP_w.reshape(-1))\n",
    "            globals()['layer{}_W_c_tmp'.format(layer_index)].append(TMP_w)  #클러스터링 용도\n",
    "\n",
    "        #---------------------------------------------------- 실루엣 기반 클러스터링 실시 -------------------------------------------------------------\n",
    "        silhoutte_score_w = []\n",
    "        transformed = globals()['layer{}_W_c_tmp'.format(layer_index)]\n",
    "\n",
    "        for clusters in range((int(local/2)-1)):       ####=>  이 범위 : 클러스터 갯수, 밑에 보면 cluster+2 있음 = 총 10개 => (0,11) = 10\n",
    "            globals()['w_layer{}_label'.format(layer_index)] = 0\n",
    "                                                            \n",
    "            kmeans_model_w = KMeans(n_clusters=clusters+2)   # 0, 1은 silhouette 에 안먹힘  \n",
    "\n",
    "            kmeans_model_w.fit(transformed)       #레이블 정의 \n",
    "\n",
    "            silhoutte_score_w.append(silhouette_score(globals()['layer{}_W_tmp'.format(layer_index)], kmeans_model_w.labels_, metric='euclidean'))  \n",
    "            \n",
    "        num_clusters_for_w = np.argmax(silhoutte_score_w)  \n",
    "        best_Sil_score_c.append(silhoutte_score_w[num_clusters_for_w])\n",
    " \n",
    "        kmeans_model_w = KMeans(n_clusters=num_clusters_for_w+2)   # 0, 1은 silhouette 에 안먹힘  \n",
    "\n",
    "        globals()['w_layer{}_label'.format(layer_index)] = kmeans_model_w.fit_predict(transformed)       # 가장 적합한 클러스터로 레이어에 속한 로컬들 레이블 정의 \n",
    "        label_list.append(globals()['w_layer{}_label'.format(layer_index)])\n",
    "\n",
    "        globals()['G{}_num_clusters_in_layer{}_w'.format(global_epoch, layer_index)] = num_clusters_for_w + 2  #나중에 몇개로 나뉘었는지 알아야 함 = num_clusters_for_w이거 그대로 하면 밑에 저게 쓸일이 많아서 global 쓰면 코딩이 너무 번잡해짐;; \n",
    "        print(\"Layer : \", layer_index, \"  클러스터 갯수 정해짐!  갯수 : \", globals()['G{}_num_clusters_in_layer{}_w'.format(global_epoch, layer_index)], num_clusters_for_w) ########################################################******************************************\n",
    "\n",
    "\n",
    "\n",
    "        #----------------------------------------------- 스케일링 및 업데이트 ---------------------------------------------------------------------\n",
    "        for num_c_w in range(num_clusters_for_w + 2): #클러스터 갯수 (num_clusters_for_w) 만큼 클러스터용 = 클러스터 갯수가 3이 나왔으면 3개를 만들어야 함\n",
    "            globals()['G{}_c{}_w_layer{}'.format(global_epoch, num_c_w, layer_index)] = 0 \n",
    "            \n",
    "            globals()['w_scaler_c{}_sum'.format(num_c_w)], globals()['int_c{}_w'.format(num_c_w)] = 0, 0 #초기화\n",
    "        \n",
    "            for i in range(1, local):\n",
    "\n",
    "                if globals()['w_layer{}_label'.format(layer_index)][i-1] == num_c_w  : # 클러스터 레이블에 맞게 (0 or 1 or 2 ..) \n",
    "                    #print(\"클러스터 레이블 check : ====== \", num_c_w)\n",
    "                    globals()['w_scaler_c{}_sum'.format(num_c_w)] = globals()['w_scaler_c{}_sum'.format(num_c_w)] + len(globals()['L{}_x_train'.format(i)])\n",
    "\n",
    "\n",
    "                    globals()['L{}_layer{}_w_c'.format(i,layer_index)][0] = np.tanh(globals()['L{}_layer{}_w_c'.format(i,layer_index)][0])\n",
    "                    globals()['L{}_layer{}_w_c'.format(i,layer_index)][1] = np.tanh(globals()['L{}_layer{}_w_c'.format(i,layer_index)][1])\n",
    "\n",
    "\n",
    "                    globals()['G{}_c{}_w_layer{}'.format(global_epoch, num_c_w, layer_index)] = globals()['G{}_c{}_w_layer{}'.format(global_epoch, num_c_w, layer_index)] + np.array(globals()['L{}_layer{}_w_c'.format(i,layer_index)])   # 일반 클러스터링\n",
    "                  \n",
    "                    globals()['int_c{}_w'.format(num_c_w)] = globals()['int_c{}_w'.format(num_c_w)] + 1\n",
    "\n",
    "            globals()['w_scaler{}'.format(num_c_w)] = len(globals()['L{}_x_train'.format(i)]) / globals()['w_scaler_c{}_sum'.format(num_c_w)]\n",
    "            globals()['G{}_c{}_w_layer{}'.format(global_epoch, num_c_w, layer_index)] = globals()['G{}_c{}_w_layer{}'.format(global_epoch, num_c_w, layer_index)] / globals()['int_c{}_w'.format(num_c_w)] #레이어 별 클러스터 자기 갯수만큼 나눠주기=평균\n",
    "\n",
    "        \n",
    "        sum_for_var_w,sum_for_var_b = 0, 0\n",
    "        \n",
    "        for sps in range(num_clusters_for_w + 2):\n",
    "            sum_for_var_w = sum_for_var_w + globals()['G{}_c{}_w_layer{}'.format(global_epoch, sps, layer_index)][0] \n",
    "            sum_for_var_b = sum_for_var_b + globals()['G{}_c{}_w_layer{}'.format(global_epoch, sps, layer_index)][1] \n",
    "        \n",
    "        sum_for_var_w = sum_for_var_w/(num_clusters_for_w + 2)\n",
    "        sum_for_var_b = sum_for_var_b/(num_clusters_for_w + 2)\n",
    "        \n",
    "\n",
    "        for i in range(1, local):\n",
    "            globals()['var_list_layer{}_w'.format(layer_index)].append(np.var(sum_for_var_w))\n",
    "            globals()['var_list_layer{}_b'.format(layer_index)].append(np.var(sum_for_var_b))\n",
    "  \n",
    "\n",
    "        VAR_final_list_w.append(globals()['var_list_layer{}_w'.format(layer_index)])\n",
    "        VAR_final_list_b.append(globals()['var_list_layer{}_b'.format(layer_index)])\n",
    "\n",
    "\n",
    "    Cluster_ACC.append(cluster_acc)\n",
    "    Cluster_LOSS.append(cluster_loss)\n",
    "    Silhouette_sc_c.append(best_Sil_score_c)\n",
    "\n",
    "    print(\"\\n\\n====================================================== One Global Epoch =====================================================================\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2XKazZ-4dgs"
   },
   "outputs": [],
   "source": [
    "#클러스터 레이어들끼리 weight 확이 => 똑같지는 않은지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uqolm_BwVC8J"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "L0_w = (G0_c0_w_layer0[0] + G0_c1_w_layer0[0] + G0_c2_w_layer0[0])/3\n",
    "L1_w = (G0_c0_w_layer2[0] + G0_c1_w_layer2[0] + G0_c2_w_layer2[0])/3\n",
    "L2_w = (G0_c0_w_layer4[0] + G0_c1_w_layer4[0] + G0_c2_w_layer4[0])/3\n",
    "L3_w = (G0_c0_w_layer6[0] + G0_c1_w_layer6[0] + G0_c2_w_layer6[0])/3\n",
    "L4_w = (G0_c0_w_layer7[0] + G0_c1_w_layer7[0] + G0_c2_w_layer7[0])/3\n",
    "\n",
    "L0_b = (G0_c0_w_layer0[1] + G0_c1_w_layer0[1] + G0_c2_w_layer0[1])/3\n",
    "L1_b = (G0_c0_w_layer2[1] + G0_c1_w_layer2[1] + G0_c2_w_layer2[1])/3\n",
    "L2_b = (G0_c0_w_layer4[1] + G0_c1_w_layer4[1] + G0_c2_w_layer4[1])/3\n",
    "L3_b = (G0_c0_w_layer6[1] + G0_c1_w_layer6[1] + G0_c2_w_layer6[1])/3\n",
    "L4_b = (G0_c0_w_layer7[1] + G0_c1_w_layer7[1] + G0_c2_w_layer7[1])/3\n",
    "\n",
    "sdd_L0_w = (G19_c0_w_layer0[0] + G19_c1_w_layer0[0] + G19_c2_w_layer0[0])/3\n",
    "sdd_L1_w = (G19_c0_w_layer2[0] + G19_c1_w_layer2[0] + G19_c2_w_layer2[0])/3\n",
    "sdd_L2_w = (G19_c0_w_layer4[0] + G19_c1_w_layer4[0] + G19_c2_w_layer4[0])/3\n",
    "sdd_L3_w = (G19_c0_w_layer6[0] + G19_c1_w_layer6[0] + G19_c2_w_layer6[0])/3\n",
    "sdd_L4_w = (G19_c0_w_layer7[0] + G19_c1_w_layer7[0] + G19_c2_w_layer7[0])/3\n",
    "\n",
    "sdd_L0_b = (G19_c0_w_layer0[1] + G19_c1_w_layer0[1] + G19_c2_w_layer0[1])/3\n",
    "sdd_L1_b = (G19_c0_w_layer2[1] + G19_c1_w_layer2[1] + G19_c2_w_layer2[1])/3\n",
    "sdd_L2_b = (G19_c0_w_layer4[1] + G19_c1_w_layer4[1] + G19_c2_w_layer4[1])/3\n",
    "sdd_L3_b = (G19_c0_w_layer6[1] + G19_c1_w_layer6[1] + G19_c2_w_layer6[1])/3\n",
    "sdd_L4_b = (G19_c0_w_layer7[1] + G19_c1_w_layer7[1] + G19_c2_w_layer7[1])/3\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.kdeplot(L1_b, label='Distribution of Parameter (Global epoch 1)', linewidth=5)\n",
    "sns.kdeplot(sdd_L1_b, label='Distribution of Parameter (Global epoch 30)', linewidth=5)\n",
    "plt.xlabel('Parameter Value', fontsize=18)\n",
    "plt.ylabel('Density', fontsize=18)\n",
    "\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 459,
     "status": "ok",
     "timestamp": 1627791491960,
     "user": {
      "displayName": "이훈민",
      "photoUrl": "",
      "userId": "00157268181097362793"
     },
     "user_tz": 240
    },
    "id": "DGhQbiXaEUfJ",
    "outputId": "7d9d04e5-e9b9-45be-8647-04287be4e014"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "L0_label0, L0_label1, L0_label2 = 0, 0, 0 \n",
    "L1_label0, L1_label1, L1_label2 = 0, 0, 0 \n",
    "L2_label0, L2_label1, L2_label2 = 0, 0, 0 \n",
    "L3_label0, L3_label1, L3_label2 = 0, 0, 0 \n",
    "L4_label0, L4_label1, L4_label2 = 0, 0, 0 \n",
    "\n",
    "\n",
    "for i in range(int(len(label_list)/5)):\n",
    "    for k in range(5):\n",
    "      for j in range(len(label_list[0])):\n",
    "          if label_list[i*5 + k][j] == 0:\n",
    "              globals()['L{}_label0'.format(k)] = globals()['L{}_label0'.format(k)] + 1 \n",
    "\n",
    "          elif label_list[i*5 + k][j] == 1:\n",
    "              globals()['L{}_label1'.format(k)] = globals()['L{}_label1'.format(k)] + 1\n",
    "\n",
    "          elif label_list[i*5 + k][j] == 2:\n",
    "              globals()['L{}_label2'.format(k)] = globals()['L{}_label2'.format(k)] + 1\n",
    "\n",
    "\n",
    "print(\"Layer 0  ==> label 0, 1, 2 갯수 : \", L0_label0, L0_label1, L0_label2)\n",
    "print(\"Layer 2  ==> label 0, 1, 2 갯수 : \", L1_label0, L1_label1, L1_label2)\n",
    "print(\"Layer 4  ==> label 0, 1, 2 갯수 : \", L2_label0, L2_label1, L2_label2)\n",
    "print(\"Layer 6  ==> label 0, 1, 2 갯수 : \", L3_label0, L3_label1, L3_label2)\n",
    "print(\"Layer 7  ==> label 0, 1, 2 갯수 : \", L4_label0, L4_label1, L4_label2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 763,
     "status": "ok",
     "timestamp": 1627791500998,
     "user": {
      "displayName": "이훈민",
      "photoUrl": "",
      "userId": "00157268181097362793"
     },
     "user_tz": 240
    },
    "id": "3mk4C9N2ADYL",
    "outputId": "82581dc2-fd24-4c0d-c5e6-107b7f2666c9"
   },
   "outputs": [],
   "source": [
    "var_changes_L0_w, var_changes_L2_w, var_changes_L4_w, var_changes_L6_w, var_changes_L7_w = [], [], [], [], [] \n",
    "var_accumulate_L0_w, var_accumulate_L2_w, var_accumulate_L4_w, var_accumulate_L6_w, var_accumulate_L7_w = [], [], [], [], [] \n",
    "\n",
    "var_changes_L0_b, var_changes_L2_b, var_changes_L4_b, var_changes_L6_b, var_changes_L7_b = [], [], [], [], [] \n",
    "var_accumulate_L0_b, var_accumulate_L2_b, var_accumulate_L4_b, var_accumulate_L6_b, var_accumulate_L7_b = [], [], [], [], [] \n",
    "\n",
    "\n",
    "for global_epoch in range(G_epoch-1):\n",
    "    for layer_index in num_layers_list:\n",
    "        sumss_w = (globals()['G{}_c{}_w_layer{}'.format(global_epoch, 0, layer_index)][0] + globals()['G{}_c{}_w_layer{}'.format(global_epoch, 1, layer_index)][0] + globals()['G{}_c{}_w_layer{}'.format(global_epoch, 2, layer_index)][0])/3\n",
    "        globals()['var_accumulate_L{}_w'.format(layer_index)].append(np.var( sumss_w ))\n",
    "        \n",
    "        sumss_b = (globals()['G{}_c{}_w_layer{}'.format(global_epoch, 0, layer_index)][1] + globals()['G{}_c{}_w_layer{}'.format(global_epoch, 1, layer_index)][1] + globals()['G{}_c{}_w_layer{}'.format(global_epoch, 2, layer_index)][1])/3\n",
    "        globals()['var_accumulate_L{}_b'.format(layer_index)].append(np.var( sumss_b ))\n",
    "        \n",
    "        \n",
    "\n",
    "for layer_index in num_layers_list:\n",
    "    for tmp_int in range(len(globals()['var_accumulate_L{}_w'.format(layer_index)])-1):\n",
    "        minus_w = globals()['var_accumulate_L{}_w'.format(layer_index)][tmp_int + 1] - globals()['var_accumulate_L{}_w'.format(layer_index)][tmp_int]\n",
    "        minus_b = globals()['var_accumulate_L{}_b'.format(layer_index)][tmp_int + 1] - globals()['var_accumulate_L{}_b'.format(layer_index)][tmp_int]        \n",
    "        globals()['var_changes_L{}_w'.format(layer_index)].append( minus_w )\n",
    "        globals()['var_changes_L{}_b'.format(layer_index)].append( minus_b )\n",
    "        \n",
    "\n",
    "#print(\"Variance Accumulate Layer 0,2,4,6,7\\n--------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "print(\"\\nVar Accumulate Weight\\n\")\n",
    "for j in num_layers_list:\n",
    "    print(globals()['var_accumulate_L{}_w'.format(j)])\n",
    "\n",
    "print(\"\\nVar Accumulate Bias\\n\")\n",
    "for j in num_layers_list:\n",
    "    print(globals()['var_accumulate_L{}_b'.format(j)])\n",
    "\n",
    "#print(\"\\n\\nVariance Change Layer 0,2,4,6,7\\n--------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "print(\"\\nVar Changes Weight\\n\")\n",
    "for j in num_layers_list:\n",
    "    print(globals()['var_changes_L{}_w'.format(j)])\n",
    "\n",
    "print(\"\\nVar Changes Bias\\n\")\n",
    "for j in num_layers_list:\n",
    "    print(globals()['var_changes_L{}_b'.format(j)])\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nNow Check the graph\\n\")\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "for j in num_layers_list:\n",
    "    plt.plot(globals()['var_accumulate_L{}_w'.format(j)])\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "for j in num_layers_list:\n",
    "    plt.plot(globals()['var_accumulate_L{}_b'.format(j)])\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "for j in num_layers_list:\n",
    "    plt.plot(globals()['var_changes_L{}_b'.format(j)])\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "for j in num_layers_list:\n",
    "    plt.plot(globals()['var_changes_L{}_b'.format(j)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "executionInfo": {
     "elapsed": 355,
     "status": "ok",
     "timestamp": 1627791511707,
     "user": {
      "displayName": "이훈민",
      "photoUrl": "",
      "userId": "00157268181097362793"
     },
     "user_tz": 240
    },
    "id": "3Z0FOMEdHTVi",
    "outputId": "fc183cde-21b1-43da-fd3b-fe11911b2f7e"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "dist_0, dist_2, dist_4, dist_6, dist_7  = [], [], [], [], []\n",
    "\n",
    "for global_epoch in range(G_epoch): #1이 맞음 = 바꾸지 말기\n",
    "    for layer_index in num_layers_list:\n",
    "        globals()['G{}_w_layer{}'.format(global_epoch, layer_index)] = 0    \n",
    "\n",
    "\n",
    "for global_epoch in range(G_epoch-1):    #2가 맞음 = 바꾸지 말기\n",
    "    for layer_index in num_layers_list:\n",
    "        globals()['G{}_w_layer{}'.format(global_epoch, layer_index)] = (globals()['G{}_c{}_w_layer{}'.format(global_epoch, 0, layer_index)] + globals()['G{}_c{}_w_layer{}'.format(global_epoch, 1, layer_index)] + globals()['G{}_c{}_w_layer{}'.format(global_epoch, 2, layer_index)])/3\n",
    "        globals()['dist_{}'.format(layer_index)].append(np.mean(distance.euclidean(globals()['G{}_w_layer{}'.format(global_epoch, layer_index)], globals()['G{}_w_layer{}'.format(global_epoch+1, layer_index)])))\n",
    "\n",
    "print(\"Distance Layer 0,2,4,6,7\\n\")\n",
    "print(dist_0)\n",
    "print(dist_2)\n",
    "print(dist_4)\n",
    "print(dist_6)\n",
    "print(dist_7, \"\\n\\nCheck the graph\\n\")\n",
    "\n",
    "plt.plot(dist_0)\n",
    "plt.plot(dist_2)\n",
    "plt.plot(dist_4)\n",
    "plt.plot(dist_6)\n",
    "plt.plot(dist_7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "executionInfo": {
     "elapsed": 289,
     "status": "ok",
     "timestamp": 1627791517140,
     "user": {
      "displayName": "이훈민",
      "photoUrl": "",
      "userId": "00157268181097362793"
     },
     "user_tz": 240
    },
    "id": "4vSuPXEKhOVJ",
    "outputId": "e1b54910-59c2-4d2a-f2c8-31eeb05ef7d1"
   },
   "outputs": [],
   "source": [
    "average_acc_cluster_list, average_loss_cluster_list = [], []\n",
    "\n",
    "\n",
    "for i in range(len(Cluster_ACC)):    \n",
    "    average_acc_cluster_list.append(np.mean(Cluster_ACC[i]))\n",
    "    average_loss_cluster_list.append(np.mean(Cluster_LOSS[i]))\n",
    "\n",
    "    \n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(average_acc_cluster_list, label='Cluster Acc')\n",
    "print(Cluster_ACC)\n",
    "print(Cluster_LOSS,\"\\n\\n\\n\\n\")\n",
    "\n",
    "plt.plot(average_loss_cluster_list, label='Cluster Loss')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7afw9VS7PwPX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMipZLNQ7Dn7KD9b2Byoc2e",
   "collapsed_sections": [],
   "name": "Test1_paper_Clustering+Non-linear.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
